{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliabui/csc408-411/blob/main/CSC411Module3OverfittingCorrection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Group Activity: How to correct for overfitting\n",
        "\n",
        "Make a copy of this notebook.\n",
        "\n",
        "In this activity you will get into groups of 4-5 people (try to mix it up with the CS and PINC students for different perspectives).\n",
        "\n",
        "Follow the instructions to correct the code in the second code cell to help with overfitting.\n",
        "\n",
        "Write down 5 modifications you made or things you learned and share them with the class."
      ],
      "metadata": {
        "id": "s6Z5xwWqieuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to use these scripts\n",
        "\n",
        "* Run the first script to see what a model without much overfitting looks like in the output.\n",
        "\n",
        "* Run the second as-is to show baseline vs. regularized/simplified models and early-stopped boosting/NN.\n",
        "\n",
        "* Inspect CV RMSE vs Test RMSE/R² and decide which model generalizes best.\n",
        "\n",
        "* Take the drop-in code below the two scripts to tune that various parts to reduce overfitting. Make as many scripts with these modifications as you want, but save each one to compare it to previous modifications. Compare results."
      ],
      "metadata": {
        "id": "3_EvaQIcjXjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Overfitting fixes demo (version-agnostic RMSE + scorer)\n",
        "# =========================\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error, r2_score, make_scorer, get_scorer\n",
        "from sklearn.dummy import DummyRegressor\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Try to use HistGradientBoosting (with early stopping); otherwise fall back to GradientBoosting\n",
        "try:\n",
        "    from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "    HAVE_HGB = True\n",
        "except Exception:\n",
        "    from sklearn.ensemble import GradientBoostingRegressor\n",
        "    HAVE_HGB = False\n",
        "\n",
        "RNG = 42\n",
        "\n",
        "# ----- Version-agnostic RMSE function -----\n",
        "def rmse(y_true, y_pred):\n",
        "    \"\"\"Return RMSE, compatible with old sklearn (no squared=False).\"\"\"\n",
        "    try:\n",
        "        return mean_squared_error(y_true, y_pred, squared=False)\n",
        "    except TypeError:\n",
        "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "# ----- Version-agnostic CV scorer for RMSE -----\n",
        "try:\n",
        "    # If the named scorer exists, use it (sklearn >= 0.22)\n",
        "    get_scorer(\"neg_root_mean_squared_error\")\n",
        "    RMSE_SCORER = \"neg_root_mean_squared_error\"\n",
        "except Exception:\n",
        "    # Otherwise make our own; greater_is_better=False => values will be NEGATED by sklearn\n",
        "    RMSE_SCORER = make_scorer(\n",
        "        lambda yt, yp: np.sqrt(mean_squared_error(yt, yp)),\n",
        "        greater_is_better=False\n",
        "    )\n",
        "\n",
        "# =========================\n",
        "# 0) Data\n",
        "# =========================\n",
        "n, d = 800, 6\n",
        "X, y = make_regression(n_samples=n, n_features=d, noise=12.0, random_state=RNG)\n",
        "# Inject a touch of nonlinearity so non-linear models have something to learn\n",
        "y = y + 0.004 * (X[:, 0] ** 3)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=RNG\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# Helpers\n",
        "# =========================\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=RNG)\n",
        "\n",
        "def eval_and_report(name, model):\n",
        "    \"\"\"\n",
        "    Do CV on train (safer generalization estimate), then refit on full train and\n",
        "    evaluate once on the held-out test set.\n",
        "    \"\"\"\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=RMSE_SCORER)\n",
        "    cv_rmse = -cv_scores.mean()   # scores are negative RMSE\n",
        "    cv_std  =  cv_scores.std()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_hat = model.predict(X_test)\n",
        "    print(f\"{name:>30s} | CV RMSE: {cv_rmse:.3f} ± {cv_std:.3f} | \"\n",
        "          f\"Test RMSE: {rmse(y_test, y_hat):.3f} | Test R²: {r2_score(y_test, y_hat):.3f}\")\n",
        "\n",
        "print(\"\\n=== Baselines & Regularized/Simpler Models ===\")\n",
        "\n",
        "# =========================\n",
        "# 1) Mean baseline (yardstick)\n",
        "# =========================\n",
        "baseline = DummyRegressor(strategy=\"mean\")\n",
        "eval_and_report(\"Mean baseline\", baseline)\n",
        "\n",
        "# =========================\n",
        "# 2) Linear + L2 (Ridge) in a leakage-safe Pipeline\n",
        "# =========================\n",
        "ridge = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", Ridge())\n",
        "])\n",
        "ridge_grid = GridSearchCV(\n",
        "    ridge,\n",
        "    param_grid={\"model__alpha\": np.logspace(-3, 3, 9)},\n",
        "    scoring=RMSE_SCORER, cv=cv, n_jobs=-1\n",
        ")\n",
        "eval_and_report(\"Ridge (alpha tuned)\", ridge_grid)\n",
        "\n",
        "# =========================\n",
        "# 3) Polynomial capacity control + L2\n",
        "# =========================\n",
        "poly_ridge = Pipeline([\n",
        "    (\"poly\", PolynomialFeatures(include_bias=False)),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", Ridge())\n",
        "])\n",
        "poly_grid = GridSearchCV(\n",
        "    poly_ridge,\n",
        "    param_grid={\n",
        "        \"poly__degree\": [1, 2, 3, 4],\n",
        "        \"model__alpha\": np.logspace(-3, 3, 7),\n",
        "    },\n",
        "    scoring=RMSE_SCORER, cv=cv, n_jobs=-1\n",
        ")\n",
        "eval_and_report(\"Poly + Ridge (deg, alpha tuned)\", poly_grid)\n",
        "\n",
        "# =========================\n",
        "# 4) k-NN — smoothness via k\n",
        "# =========================\n",
        "knn = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", KNeighborsRegressor())\n",
        "])\n",
        "knn_grid = GridSearchCV(\n",
        "    knn,\n",
        "    param_grid={\"model__n_neighbors\": [3, 5, 8, 15, 25]},\n",
        "    scoring=RMSE_SCORER, cv=cv, n_jobs=-1\n",
        ")\n",
        "eval_and_report(\"k-NN (k tuned)\", knn_grid)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nTip: pick the **simplest** model that consistently beats the mean baseline and linear Ridge on CV and test.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRX4fMAhk8WX",
        "outputId": "491b2756-ea23-4cc7-e2bd-20f7ea03e7b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Baselines & Regularized/Simpler Models ===\n",
            "                 Mean baseline | CV RMSE: 123.876 ± 8.633 | Test RMSE: 123.963 | Test R²: -0.025\n",
            "           Ridge (alpha tuned) | CV RMSE: 12.048 ± 0.545 | Test RMSE: 11.485 | Test R²: 0.991\n",
            "Poly + Ridge (deg, alpha tuned) | CV RMSE: 12.048 ± 0.545 | Test RMSE: 11.483 | Test R²: 0.991\n",
            "                k-NN (k tuned) | CV RMSE: 50.742 ± 3.329 | Test RMSE: 46.432 | Test R²: 0.856\n",
            "\n",
            "Tip: pick the **simplest** model that consistently beats the mean baseline and linear Ridge on CV and test.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How to interpret results:\n",
        "\n",
        "**Baseline is terrible (yardstick).**\n",
        "\n",
        "* Mean baseline Test RMSE ≈ 124, Test R² -0.025 → predicting the average for everyone is useless. Your target's spread is roughly this size, so anything meaningful must beat ~124 RMSE.\n",
        "\n",
        "**Linear Ridge wins (and generalizes).**\n",
        "* Ridge (alpha tuned) CV RMSE 12.05 ± 0.55, Test RMSE 11.49, Test R² 0.991.\n",
        "\n",
        "**Small CV±std → stable across folds.**\n",
        "\n",
        "* Test ≈ CV (slightly better) → no overfitting; the test split is just a bit easier.\n",
        "\n",
        " **“Poly + Ridge” added no value.**\n",
        "\n",
        "* Same numbers as Ridge → the grid likely chose degree = 1 (i.e., it collapsed to linear). So curvature wasn't needed.\n",
        "\n",
        "**k-NN underperforms here.**\n",
        "* k-NN Test RMSE 46.4 (R² 0.856). It learns something, but far worse than Ridge—likely higher variance on this problem."
      ],
      "metadata": {
        "id": "R6VvVNphSJri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An Overfitted Model (Intentionally)"
      ],
      "metadata": {
        "id": "xUOqnscZXD8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# OVERFITTING VERSION\n",
        "# =========================\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score, get_scorer, make_scorer\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, GradientBoostingRegressor\n",
        "\n",
        "RNG = 42\n",
        "HAVE_HGB = True  # set False if your sklearn lacks HistGradientBoostingRegressor\n",
        "\n",
        "# ----- Version-agnostic RMSE function -----\n",
        "def rmse(y_true, y_pred):\n",
        "    \"\"\"Return RMSE, compatible with old sklearn (no squared=False).\"\"\"\n",
        "    try:\n",
        "        return mean_squared_error(y_true, y_pred, squared=False)\n",
        "    except TypeError:\n",
        "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "# ----- Version-agnostic CV scorer for RMSE -----\n",
        "try:\n",
        "    get_scorer(\"neg_root_mean_squared_error\")\n",
        "    RMSE_SCORER = \"neg_root_mean_squared_error\"\n",
        "except Exception:\n",
        "    RMSE_SCORER = make_scorer(\n",
        "        lambda yt, yp: np.sqrt(mean_squared_error(yt, yp)),\n",
        "        greater_is_better=False\n",
        "    )\n",
        "\n",
        "# =========================\n",
        "# 0) Data (keep the same)\n",
        "# =========================\n",
        "n, d = 800, 6\n",
        "X, y = make_regression(n_samples=n, n_features=d, noise=12.0, random_state=RNG)\n",
        "y = y + 0.004 * (X[:, 0] ** 3)\n",
        "\n",
        "# (Leave normal split; overfitting will still show up clearly)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=RNG\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# Helpers\n",
        "# =========================\n",
        "# Use a light CV (few folds) to make it easier for high-capacity models to slip through.\n",
        "cv = KFold(n_splits=3, shuffle=False)  # intentionally weaker CV\n",
        "\n",
        "def eval_and_report(name, model):\n",
        "    \"\"\"\n",
        "    Do CV on train (for reference), then fit the full train and report Train vs Test.\n",
        "    Overfitting will show as Train RMSE << Test RMSE.\n",
        "    \"\"\"\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=RMSE_SCORER)\n",
        "    cv_rmse = -cv_scores.mean()\n",
        "    cv_std  =  cv_scores.std()\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_hat_test  = model.predict(X_test)\n",
        "    y_hat_train = model.predict(X_train)\n",
        "\n",
        "    print(f\"{name:>35s} | CV RMSE: {cv_rmse:.3f} ± {cv_std:.3f} | \"\n",
        "          f\"Train RMSE: {rmse(y_train, y_hat_train):.3f} | \"\n",
        "          f\"Test RMSE: {rmse(y_test,  y_hat_test):.3f} | \"\n",
        "          f\"Test R²: {r2_score(y_test, y_hat_test):.3f}\")\n",
        "\n",
        "print(\"\\n=== Baseline ===\")\n",
        "baseline = DummyRegressor(strategy=\"mean\")\n",
        "eval_and_report(\"Mean baseline\", baseline)\n",
        "\n",
        "print(\"\\n=== INTENTIONALLY OVERFITTED MODELS ===\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1) High-degree Polynomial Regression with ~no regularization\n",
        "#    (explodes feature space; LinearRegression memorizes patterns)\n",
        "# ---------------------------------------------------------\n",
        "poly_ols = Pipeline([\n",
        "    (\"poly\",   PolynomialFeatures(degree=7, include_bias=False)),  # very high capacity\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\",  LinearRegression())                                  # no L2 penalty\n",
        "])\n",
        "eval_and_report(\"Poly degree=7 + OLS (no reg)\", poly_ols)\n",
        "\n",
        "# (Optional: a barely-regularized Ridge is also very high-capacity)\n",
        "weak_ridge = Pipeline([\n",
        "    (\"poly\",   PolynomialFeatures(degree=7, include_bias=False)),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\",  Ridge(alpha=1e-9))   # virtually no shrinkage\n",
        "])\n",
        "eval_and_report(\"Poly degree=7 + Ridge(alpha≈0)\", weak_ridge)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2) k-NN with k=1 (each point predicts itself on train ⇒ ~0 train error)\n",
        "# ---------------------------------------------------------\n",
        "knn_1 = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\",  KNeighborsRegressor(n_neighbors=1))\n",
        "])\n",
        "eval_and_report(\"k-NN (k=1)\", knn_1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nNote: You should see much lower Train RMSE than Test RMSE for these models — a hallmark of overfitting.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWFJ2zkOUoUq",
        "outputId": "1d5923d5-8a09-498e-9489-fa81937099bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Baseline ===\n",
            "                      Mean baseline | CV RMSE: 124.189 ± 1.186 | Train RMSE: 124.117 | Test RMSE: 123.963 | Test R²: -0.025\n",
            "\n",
            "=== INTENTIONALLY OVERFITTED MODELS ===\n",
            "       Poly degree=7 + OLS (no reg) | CV RMSE: 327.297 ± 91.116 | Train RMSE: 0.000 | Test RMSE: 350.770 | Test R²: -7.208\n",
            "     Poly degree=7 + Ridge(alpha≈0) | CV RMSE: 327.297 ± 91.116 | Train RMSE: 0.000 | Test RMSE: 350.770 | Test R²: -7.208\n",
            "                         k-NN (k=1) | CV RMSE: 60.826 ± 3.938 | Train RMSE: 0.000 | Test RMSE: 57.603 | Test R²: 0.779\n",
            "\n",
            "Note: You should see much lower Train RMSE than Test RMSE for these models — a hallmark of overfitting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#What went wrong in the overfitting code\n",
        "\n",
        "##Yardstick\n",
        "\n",
        "* Mean baseline — CV ≈ 124, Test ≈ 124, R² ≈ –0.03 (by definition this is the “do nothing but predict the mean” level).\n",
        "\n",
        "##Intentionally overfit models\n",
        "\n",
        "* Poly degree=7 + OLS (no reg) and Poly degree=7 + Ridge(α≈0)\n",
        "\n",
        "  * Train RMSE = 0.000 (memorized the training set).\n",
        "\n",
        "  * CV RMSE ≈ 327 ± 91 and Test RMSE ≈ 351, R² = –7.21 → catastrophic overfit (far worse than baseline).\n",
        "\n",
        "  * The huge CV std (±91) shows the model is unstable across folds. Discard these settings.\n",
        "\n",
        "* k-NN (k=1)\n",
        "\n",
        "  * Train RMSE = 0.000 (each point predicts itself).\n",
        "\n",
        "  * CV ≈ 60.8 ± 3.9, Test ≈ 57.6, R² ≈ 0.78 → Strong test performance but high variance model (classic 1-NN).\n",
        "\n",
        "  * Safer fix: raise k (e.g., 15–40) and re-CV; expect a small increase in Test RMSE with better stability.\n",
        "\n",
        "\n",
        "##Takeaways\n",
        "\n",
        "* The polynomial models are clear, textbook overfitting (zero train error, terrible CV/Test, negative R²).\n",
        "\n",
        "* 1-NN overfits the train but still generalizes reasonably on this split; prefer larger k to reduce variance.\n",
        "\n",
        "\n",
        "##What to change (quick fixes)\n",
        "\n",
        "* Poly: lower degree (≤3) and add Ridge with larger α.\n",
        "\n",
        "* k-NN: increase k (15–40).\n",
        "\n",
        "\n",
        "* Use stronger CV (e.g., RepeatedKFold) and pick the simplest model whose CV mean ± std beats Ridge and the baseline."
      ],
      "metadata": {
        "id": "t3xf0Hx4X47k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#What to change to correct for overfitting\n",
        "\n",
        "If CV/test error is higher than train error, you need to **reduce model capacity** or **increase regularization**. Down below are drop-in edits you can make in the above script. Experiment and pick the ones that apply to the model overfitting.\n",
        "\n",
        "**Note**: make sure that you leave the original code in your notebook. Copy the cell instead of tuning it in place, so that you have multiple versions of your model to compare. Take as much room as you need below."
      ],
      "metadata": {
        "id": "SuG8WQD8R3TI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##0) Stronger, more stable CV (replace current cv)\n",
        "\n",
        "This stabilizes CV and pushes the search toward simpler settings"
      ],
      "metadata": {
        "id": "O1oqY9pmTW_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stronger CV so tuning prefers simpler, more general models\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=RNG)\n",
        "\n"
      ],
      "metadata": {
        "id": "abfmeZ2RS-TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1) Polynomial regression --> cap degree + add L2(Ridge)"
      ],
      "metadata": {
        "id": "c2Ja-4D1TjWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace the poly degree=7 OLS and near-zero Ridge with this tuned, regularized version\n",
        "poly_ridge_reg = Pipeline([\n",
        "    (\"poly\",   PolynomialFeatures(include_bias=False)),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\",  Ridge())\n",
        "])\n",
        "\n",
        "poly_ridge_grid = GridSearchCV(\n",
        "    poly_ridge_reg,\n",
        "    param_grid={\n",
        "        \"poly__degree\": [1, 2, 3],               # cap model capacity\n",
        "        \"model__alpha\": np.logspace(-2, 3, 8)    # meaningful L2 shrinkage\n",
        "    },\n",
        "    scoring=RMSE_SCORER, cv=cv, n_jobs=-1\n",
        ")\n",
        "\n",
        "eval_and_report(\"Poly + Ridge (regularized, tuned)\", poly_ridge_grid)\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "id": "oF8uJ8DNS-Rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2)k-NN (smooth neigborhoods, bigger k)\n",
        "\n",
        "**Why**: larger k = smoother, less overfit"
      ],
      "metadata": {
        "id": "ZEputyRuSbXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace k=1 with a tuned, smoother k\n",
        "knn_reg = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\",  KNeighborsRegressor())\n",
        "])\n",
        "\n",
        "knn_grid = GridSearchCV(\n",
        "    knn_reg,\n",
        "    param_grid={\n",
        "        \"model__n_neighbors\": [8, 15, 25, 40, 60],   # larger k reduces variance\n",
        "        \"model__weights\": [\"uniform\", \"distance\"]    # distance weighting often helps\n",
        "    },\n",
        "    scoring=RMSE_SCORER, cv=cv, n_jobs=-1\n",
        ")\n",
        "\n",
        "eval_and_report(\"k-NN (smoothed, tuned)\", knn_grid)\n",
        "\n"
      ],
      "metadata": {
        "id": "K-FTGuhqT2ZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3) Sanity Checks (simple but effective)\n",
        "\n",
        "* Prefer the smallest model whose CV mean ± std beats simpler baselines.\n",
        "\n",
        "* If CV improves but test doesn’t, tighten the settings above (shallower/smoother/more regularized) or increase data.\n",
        "\n",
        "* Plot your validation curve (error vs degree/depth/etc.) and stop at the U-shape minimum.\n",
        "\n"
      ],
      "metadata": {
        "id": "H3-RPIdtUdxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What all of these do\n",
        "\n",
        "* RepeatedKFold stabilizes CV and nudges selection toward simpler models.\n",
        "\n",
        "* Poly+Ridge limits polynomial degree and adds meaningful L2 shrinkage.\n",
        "\n",
        "* k-NN uses a larger k (and optionally distance weights) to reduce variance.\n",
        "\n",
        "\n",
        "Run with these changes and you should see Train RMSE ≈ CV/Test RMSE and improved stability (smaller CV std)."
      ],
      "metadata": {
        "id": "Y4XwdLQnZPof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start your experimentation here\n",
        "\n",
        "Below, make as many code cells as you need to. Copy and paste the overfitting code and then make the drop-in fixes one by one, replacing the parts systematically so that you can compare the results. Make one final version with all the fixes you decide to use, and check your final results to see if there is still overfitting.\n",
        "\n",
        "Then make a text cell to describe what you changed, why, and how it affected the fit of the model."
      ],
      "metadata": {
        "id": "nsuzg5pqZW7V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nBxVEuFaZ4oY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ytrcs7aoZ4lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZkWlvmZ2Z4gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kM7UpsANZ4dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4ajwDCX2Z4aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M-Hak_01Z4XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZxRS6pudZ4UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X6cIzITwZ4Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p_bhMjalZ3_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a0azicGJZ38l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Ucagwd7Z3wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4Ioci3NHZ3ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AW1lbA1sZ3Ey"
      }
    }
  ]
}