{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliabui/csc408-411/blob/main/Mod3_Logistic_Regression_Log_Loss%20%23411.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This code:**\n",
        "\n",
        "* makes a synthetic binary dataset\n",
        "\n",
        "* fits LogisticRegression inside a Pipeline(StandardScaler → LogisticRegression)\n",
        "\n",
        "* tunes for log loss with cross-validated GridSearchCV,\n",
        "\n",
        "* reports test log loss (and a few extra metrics for context).\n",
        "\n",
        "**Notes**\n",
        "\n",
        "* Why scoring='neg_log_loss'? GridSearchCV maximizes the score, so we use the negative log loss to minimize log loss.\n",
        "\n",
        "* Regularization: C is the inverse of regularization strength (smaller C → stronger regularization). Searching L1 vs L2 and class_weight can materially change log loss, especially with imbalance.\n",
        "\n",
        "* Protocol: tune via CV on the train split only, then report metrics once on the test split for an unbiased estimate."
      ],
      "metadata": {
        "id": "7zbq3qUKZjaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3 is where we tune**\n",
        "\n",
        "* Try different values for the parameter grid in gridsearch on line 47\n",
        "\n",
        "* Options for scoring on line 56\n",
        "\n",
        "  * \"roc_auc\" - ROC AUC (binary)\n",
        "\n",
        "  * \"roc_auc_ovr\", \"roc_auc_ovo\" - multiclass variants\n",
        "\n",
        "  * \"average_precision\" - PR-AUC (great for rare positives)\n",
        "\n",
        "**Rule of thumb**\n",
        "\n",
        "* Want well-calibrated probabilities → neg_log_loss, neg_brier_score.\n",
        "\n",
        "* Care about ranking under imbalance → average_precision (or roc_auc).\n",
        "\n",
        "* Reporting hard labels at a fixed threshold → f1/precision/recall (pick what matches your objective)."
      ],
      "metadata": {
        "id": "LE7QsRQdavwE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JC9fS1tSZP8V",
        "outputId": "4decb7ef-5e64-4945-8c18-d255f3e20a41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline (always predict prevalence=0.306) | Test log loss: 0.6157\n",
            "\n",
            "Best by CV (mean ± std over folds):\n",
            "  neg_log_loss: -0.2333  (C=0.31622776601683794, penalty=l1, class_weight=None)\n",
            "\n",
            "Test set performance (unbiased):\n",
            "  Log loss:   0.2361  (lower is better)\n",
            "  ROC AUC:    0.9537\n",
            "  Brier score:0.0625\n",
            "  Accuracy@:0.5 threshold: 0.9233\n",
            "\n",
            "Top coefficients (absolute value):\n",
            "  x02: coef= 3.0580\n",
            "  x15: coef= 1.3475\n",
            "  x05: coef=-0.6580\n",
            "  x17: coef= 0.2861\n",
            "  x12: coef=-0.2172\n",
            "  x00: coef= 0.1677\n",
            "  x16: coef=-0.1109\n",
            "  x19: coef= 0.0806\n",
            "  x03: coef= 0.0554\n",
            "  x13: coef=-0.0532\n"
          ]
        }
      ],
      "source": [
        "# Logistic regression tuned for log loss on a synthetic dataset\n",
        "# -------------------------------------------------------------\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score, brier_score_loss\n",
        "\n",
        "RNG = 42\n",
        "\n",
        "# 1) Synthetic data (slightly imbalanced; a bit of label noise)\n",
        "X, y = make_classification(\n",
        "    n_samples=4000,\n",
        "    n_features=20,\n",
        "    n_informative=6,\n",
        "    n_redundant=4,\n",
        "    class_sep=1.2,\n",
        "    flip_y=0.03,\n",
        "    weights=[0.7, 0.3],   # ~30% positive class\n",
        "    random_state=RNG\n",
        ")\n",
        "\n",
        "# Hold out a test set once; do all tuning by CV on the train split only\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, stratify=y, random_state=RNG\n",
        ")\n",
        "\n",
        "# 2) Baseline (predict the base rate for everyone) — yardstick for log loss\n",
        "p_base = np.full_like(y_test, fill_value=y_train.mean(), dtype=float)\n",
        "print(f\"Baseline (always predict prevalence={y_train.mean():.3f}) \"\n",
        "      f\"| Test log loss: {log_loss(y_test, p_base):.4f}\")\n",
        "\n",
        "# 3) Pipeline + grid search (smaller C = stronger L1/L2 regularization)\n",
        "pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"logreg\", LogisticRegression(\n",
        "        solver=\"liblinear\",    # supports L1 and L2 for binary problems\n",
        "        max_iter=1000,\n",
        "        random_state=RNG\n",
        "    ))\n",
        "])\n",
        "#Tune C with logreg_C\n",
        "param_grid = {\n",
        "    \"logreg__penalty\": [\"l2\", \"l1\"],\n",
        "    \"logreg__C\": np.logspace(-3, 3, 13),  # ← Grid of C values to try\n",
        "    \"logreg__class_weight\": [None, \"balanced\"]  # try with/without imbalance handling\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RNG)\n",
        "\n",
        "gs = GridSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"neg_log_loss\",    # <-- tune for log loss\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    refit=True,                # refit on full training set using best params\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "gs.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nBest by CV (mean ± std over folds):\")\n",
        "print(f\"  neg_log_loss: {gs.best_score_:.4f}  \"\n",
        "      f\"(C={gs.best_params_['logreg__C']}, penalty={gs.best_params_['logreg__penalty']}, \"\n",
        "      f\"class_weight={gs.best_params_['logreg__class_weight']})\")\n",
        "\n",
        "# 4) One-time evaluation on the untouched test set\n",
        "proba_test = gs.predict_proba(X_test)[:, 1]\n",
        "logloss_test = log_loss(y_test, proba_test)\n",
        "rocauc_test  = roc_auc_score(y_test, proba_test)\n",
        "brier_test   = brier_score_loss(y_test, proba_test)\n",
        "acc_test     = accuracy_score(y_test, proba_test >= 0.5)  # threshold shown just for reference\n",
        "\n",
        "print(\"\\nTest set performance (unbiased):\")\n",
        "print(f\"  Log loss:   {logloss_test:.4f}  (lower is better)\")\n",
        "print(f\"  ROC AUC:    {rocauc_test:.4f}\")\n",
        "print(f\"  Brier score:{brier_test:.4f}\")\n",
        "print(f\"  Accuracy@:0.5 threshold: {acc_test:.4f}\")\n",
        "\n",
        "# Optional: show the learned coefficients (after scaling) for interpretability\n",
        "clf = gs.best_estimator_.named_steps[\"logreg\"]\n",
        "print(\"\\nTop coefficients (absolute value):\")\n",
        "coef = clf.coef_.ravel()\n",
        "top = np.argsort(np.abs(coef))[::-1][:10]\n",
        "for j in top:\n",
        "    print(f\"  x{j:02d}: coef={coef[j]: .4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###How to interpret the results\n",
        "\n",
        "1) Baseline\n",
        "\n",
        "    * What it is: log loss if you predict the training prevalence for everyone (no features).\n",
        "\n",
        "    * How to use it: your model's test log loss must be lower than baseline. If it's close or worse → your features/model aren't adding value.\n",
        "\n",
        "2) “Best by CV …”\n",
        "\n",
        "    * neg_log_loss: GridSearchCV maximizes this, so the CV log loss ≈ -neg_log_loss.\n",
        "\n",
        "    * Example: neg_log_loss = -0.2333 ⇒ CV log loss ≈ 0.2333.\n",
        "\n",
        "    * mean ± std: mean over folds and its variability.\n",
        "\n",
        "    * Small std → stable across splits. Large std → results depend on the split (try more data/features or stronger regularization).\n",
        "\n",
        "    * Best params (C, penalty, class_weight):\n",
        "\n",
        "    * Smaller C = stronger regularization.\n",
        "\n",
        "    * penalty='l1' often zeros many coefficients (feature selection); 'l2' shrinks them smoothly.\n",
        "\n",
        "    * class_weight='balanced' helps when positives are rare.\n",
        "\n",
        "3) Test set performance (unbiased)\n",
        "\n",
        "Read these on the held-out test only after tuning:\n",
        "\n",
        "  * Log loss (lower is better): measures probability quality.\n",
        "\n",
        "  * Compare to baseline log loss and to CV log loss.\n",
        "\n",
        "    * Close to CV → good generalization. Much worse → overfitting or leakage in CV.\n",
        "\n",
        "    * ROC AUC (higher is better): ranking ability; 0.5 ≈ random, ≥0.8 strong.\n",
        "\n",
        "    * Brier score (lower is better): calibration/mean squared error of probabilities; complements log loss.\n",
        "\n",
        "  * Accuracy@0.5: accuracy after converting probabilities to labels at threshold 0.5.\n",
        "\n",
        "      *Useful for a quick read but can mislead under class imbalance. If positives are rare, also check precision/recall or PR-AUC.\n",
        "\n",
        "  * Sanity check: If test log loss ≫ CV log loss, or AUC drops a lot, revisit splits, leakage, or regularization strength.\n",
        "\n",
        "4) Top coefficients\n",
        "\n",
        "  * Sign = direction (positive increases odds of class 1; negative decreases).\n",
        "\n",
        "  * Magnitude = influence after scaling (the pipeline standardizes features).\n",
        "\n",
        "  * With L1, expect many near-zero weights (implicit feature selection).\n",
        "  * Use these for interpretability, not as proof of causation.\n",
        "\n",
        "5) What to do next (based on what you see)\n",
        "\n",
        "  * Beating baseline but high log loss: add/engineer features; try Elastic Net; check calibration (isotonic/Platt) if probabilities seem off.\n",
        "\n",
        "  * High variance across folds: increase regularization (smaller C), simplify features, or get more data.\n",
        "\n",
        "  * Good AUC but mediocre log loss/Brier: ranking is fine, probabilities may be miscalibrated → consider calibration.\n",
        "\n",
        "  * Accuracy looks great, AUC/PR-AUC mediocre: threshold may be flattering due to imbalance—optimize threshold on validation for the metric that matches your costs."
      ],
      "metadata": {
        "id": "hbmpo-6Ybm63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5UHVXTFcfQAm"
      }
    }
  ]
}